{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('juliet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>testcase_ID</th>\n",
       "      <th>filename</th>\n",
       "      <th>code</th>\n",
       "      <th>flaw</th>\n",
       "      <th>flaw_loc</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>61940</td>\n",
       "      <td>000/061/940/CWE114_Process_Control__w32_char_c...</td>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "      <td>CWE-114</td>\n",
       "      <td>121</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61941</td>\n",
       "      <td>000/061/941/CWE114_Process_Control__w32_char_c...</td>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "      <td>CWE-114</td>\n",
       "      <td>124</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>61942</td>\n",
       "      <td>000/061/942/CWE114_Process_Control__w32_char_c...</td>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "      <td>CWE-114</td>\n",
       "      <td>124</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>61943</td>\n",
       "      <td>000/061/943/CWE114_Process_Control__w32_char_c...</td>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "      <td>CWE-114</td>\n",
       "      <td>131</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>61944</td>\n",
       "      <td>000/061/944/CWE114_Process_Control__w32_char_c...</td>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "      <td>CWE-114</td>\n",
       "      <td>131</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210361</th>\n",
       "      <td>105178</td>\n",
       "      <td>-248721</td>\n",
       "      <td>000/248/721/CWE78_OS_Command_Injection__wchar_...</td>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "      <td>CWE-078</td>\n",
       "      <td>137</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210362</th>\n",
       "      <td>105179</td>\n",
       "      <td>-248722</td>\n",
       "      <td>000/248/722/CWE78_OS_Command_Injection__wchar_...</td>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "      <td>CWE-078</td>\n",
       "      <td>137</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210363</th>\n",
       "      <td>105180</td>\n",
       "      <td>-248722</td>\n",
       "      <td>000/248/722/CWE78_OS_Command_Injection__wchar_...</td>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "      <td>CWE-078</td>\n",
       "      <td>137</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210364</th>\n",
       "      <td>105181</td>\n",
       "      <td>-248722</td>\n",
       "      <td>000/248/722/CWE78_OS_Command_Injection__wchar_...</td>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "      <td>CWE-078</td>\n",
       "      <td>137</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210365</th>\n",
       "      <td>105182</td>\n",
       "      <td>-248722</td>\n",
       "      <td>000/248/722/CWE78_OS_Command_Injection__wchar_...</td>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "      <td>CWE-078</td>\n",
       "      <td>137</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210366 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  testcase_ID  \\\n",
       "0                0        61940   \n",
       "1                1        61941   \n",
       "2                2        61942   \n",
       "3                3        61943   \n",
       "4                4        61944   \n",
       "...            ...          ...   \n",
       "210361      105178      -248721   \n",
       "210362      105179      -248722   \n",
       "210363      105180      -248722   \n",
       "210364      105181      -248722   \n",
       "210365      105182      -248722   \n",
       "\n",
       "                                                 filename  \\\n",
       "0       000/061/940/CWE114_Process_Control__w32_char_c...   \n",
       "1       000/061/941/CWE114_Process_Control__w32_char_c...   \n",
       "2       000/061/942/CWE114_Process_Control__w32_char_c...   \n",
       "3       000/061/943/CWE114_Process_Control__w32_char_c...   \n",
       "4       000/061/944/CWE114_Process_Control__w32_char_c...   \n",
       "...                                                   ...   \n",
       "210361  000/248/721/CWE78_OS_Command_Injection__wchar_...   \n",
       "210362  000/248/722/CWE78_OS_Command_Injection__wchar_...   \n",
       "210363  000/248/722/CWE78_OS_Command_Injection__wchar_...   \n",
       "210364  000/248/722/CWE78_OS_Command_Injection__wchar_...   \n",
       "210365  000/248/722/CWE78_OS_Command_Injection__wchar_...   \n",
       "\n",
       "                                                     code     flaw  flaw_loc  \\\n",
       "0       /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...  CWE-114       121   \n",
       "1       /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...  CWE-114       124   \n",
       "2       /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...  CWE-114       124   \n",
       "3       /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...  CWE-114       131   \n",
       "4       /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...  CWE-114       131   \n",
       "...                                                   ...      ...       ...   \n",
       "210361  /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...  CWE-078       137   \n",
       "210362  /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...  CWE-078       137   \n",
       "210363  /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...  CWE-078       137   \n",
       "210364  /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...  CWE-078       137   \n",
       "210365  /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...  CWE-078       137   \n",
       "\n",
       "          bug  \n",
       "0       False  \n",
       "1       False  \n",
       "2       False  \n",
       "3       False  \n",
       "4       False  \n",
       "...       ...  \n",
       "210361   True  \n",
       "210362   True  \n",
       "210363   True  \n",
       "210364   True  \n",
       "210365   True  \n",
       "\n",
       "[210366 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=true_label_df = df[df['bug'] == True].copy()\n",
    "new_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer,BertLMHeadModel,BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',pad_token=\"[PAD]\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_comments(code):\n",
    "    # Regular expression to match C and C++ comments (single-line and multi-line)\n",
    "    pattern = r'/\\*.*?\\*/|//.*?$'\n",
    "\n",
    "    # Remove comments using regex\n",
    "    cleaned_code = re.sub(pattern, '', code, flags=re.MULTILINE | re.DOTALL)\n",
    "\n",
    "    return cleaned_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n#include \"std_testcase.h\"\\n\\n#include <wchar.h>\\n\\n#ifdef _WIN32\\n#include <winsock2.h>\\n#include <windows.h>\\n#include <direct.h>\\n#pragma comment(lib, \"ws2_32\") \\n#define CLOSE_SOCKET closesocket\\n#else \\n#include <sys/types.h>\\n#include <sys/socket.h>\\n#include <netinet/in.h>\\n#include <arpa/inet.h>\\n#include <unistd.h>\\n#define INVALID_SOCKET -1\\n#define SOCKET_ERROR -1\\n#define CLOSE_SOCKET close\\n#define SOCKET int\\n#endif\\n\\n#define TCP_PORT 27015\\n#define IP_ADDRESS \"127.0.0.1\"\\n\\n\\n#ifndef OMITBAD\\n\\nvoid CWE114_Process_Control__w32_char_connect_socket_01_bad()\\n{\\n    char * data;\\n    char dataBuffer[100] = \"\";\\n    data = dataBuffer;\\n    {\\n#ifdef _WIN32\\n        WSADATA wsaData;\\n        int wsaDataInit = 0;\\n#endif\\n        int recvResult;\\n        struct sockaddr_in service;\\n        char *replace;\\n        SOCKET connectSocket = INVALID_SOCKET;\\n        size_t dataLen = strlen(data);\\n        do\\n        {\\n#ifdef _WIN32\\n            if (WSAStartup(MAKEWORD(2,2), &wsaData) != NO_ERROR)\\n            {\\n                break;\\n            }\\n            wsaDataInit = 1;\\n#endif\\n            \\n            connectSocket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);\\n            if (connectSocket == INVALID_SOCKET)\\n            {\\n                break;\\n            }\\n            memset(&service, 0, sizeof(service));\\n            service.sin_family = AF_INET;\\n            service.sin_addr.s_addr = inet_addr(IP_ADDRESS);\\n            service.sin_port = htons(TCP_PORT);\\n            if (connect(connectSocket, (struct sockaddr*)&service, sizeof(service)) == SOCKET_ERROR)\\n            {\\n                break;\\n            }\\n            \\n            \\n            recvResult = recv(connectSocket, (char *)(data + dataLen), sizeof(char) * (100 - dataLen - 1), 0);\\n            if (recvResult == SOCKET_ERROR || recvResult == 0)\\n            {\\n                break;\\n            }\\n            \\n            data[dataLen + recvResult / sizeof(char)] = \\'\\\\0\\';\\n            \\n            replace = strchr(data, \\'\\\\r\\');\\n            if (replace)\\n            {\\n                *replace = \\'\\\\0\\';\\n            }\\n            replace = strchr(data, \\'\\\\n\\');\\n            if (replace)\\n            {\\n                *replace = \\'\\\\0\\';\\n            }\\n        }\\n        while (0);\\n        if (connectSocket != INVALID_SOCKET)\\n        {\\n            CLOSE_SOCKET(connectSocket);\\n        }\\n#ifdef _WIN32\\n        if (wsaDataInit)\\n        {\\n            WSACleanup();\\n        }\\n#endif\\n    }\\n    {\\n        HMODULE hModule;\\n        \\n        hModule = LoadLibraryA(data);\\n        if (hModule != NULL)\\n        {\\n            FreeLibrary(hModule);\\n            printLine(\"Library loaded and freed successfully\");\\n        }\\n        else\\n        {\\n            printLine(\"Unable to load library\");\\n        }\\n    }\\n}\\n\\n#endif \\n\\n\\n\\n\\n#ifdef INCLUDEMAIN\\n\\nint main(int argc, char * argv[])\\n{\\n    \\n    srand( (unsigned)time(NULL) );\\n#ifndef OMITBAD\\n    printLine(\"Calling bad()...\");\\n    CWE114_Process_Control__w32_char_connect_socket_01_bad();\\n    printLine(\"Finished bad()\");\\n#endif \\n    return 0;\\n}\\n\\n#endif\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_comments(new_df[\"code\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4344"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[\"code\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 256\n",
    "code = new_df[\"code\"][0]\n",
    "code_chunks = [code[i:i + max_seq_length] for i in range(0, len(code), max_seq_length)]\n",
    "\n",
    "for chunk in code_chunks:\n",
    "    inputs = tokenizer(chunk, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1013,  1008,  6534,  6721,  2791,  1008,  1013,  5034,  5685,\n",
       "          1006,  1006, 27121,  1007,  2051,  1006, 19701,  1007,  1007,  1025,\n",
       "          1001,  2065, 13629,  2546, 18168,  4183,  9024,  6140,  4179,  1006,\n",
       "          1000,  4214,  2919,  1006,  1007,  1012,  1012,  1012,  1000,  1007,\n",
       "          1025, 19296,  2063, 14526,  2549,  1035,  2832,  1035,  2491,  1035,\n",
       "          1035,  1059, 16703,  1035, 25869,  1035,  7532,  1035, 22278,  1035,\n",
       "          5890,  1035,  2919,  1006,  1007,  1025,  6140,  4179,  1006,  1000,\n",
       "          2736,  2919,  1006,  1007,  1000,  1007,  1025,  1001,  2203, 10128,\n",
       "          1013,  1008, 18168,  4183,  9024,  1008,  1013,  2709,  1014,  1025,\n",
       "          1065,  1001,  2203, 10128,   102]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup,TrainingArguments,Trainer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VulnerabilityDataset(Dataset):\n",
    "    def __init__(self, data, max_seq_length=128, max_token_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.max_token_length = max_token_length\n",
    "\n",
    "    def remove_comments(self, code):\n",
    "        # Regular expression to match C and C++ comments (single-line and multi-line)\n",
    "        pattern = r'/\\*.*?\\*/|//.*?$'\n",
    "\n",
    "        # Remove comments using regex\n",
    "        cleaned_code = re.sub(pattern, '', code, flags=re.MULTILINE | re.DOTALL)\n",
    "\n",
    "        return cleaned_code\n",
    "\n",
    "    def tokenize_long_code(self, long_code):\n",
    "        # Split the long code into smaller segments based on max_token_length\n",
    "        segments = [long_code[i:i+self.max_token_length] for i in range(0, len(long_code), self.max_token_length)]\n",
    "\n",
    "        # Tokenize each segment separately and pad them to the same length\n",
    "        tokenized_segments = []\n",
    "        max_length = 0\n",
    "        for segment in segments:\n",
    "            tokens = self.tokenizer.encode(segment, return_tensors=\"pt\", truncation=True)\n",
    "            tokenized_segments.append(tokens)\n",
    "            max_length = max(max_length, tokens.size(1))\n",
    "\n",
    "        # Pad the segments to the same length\n",
    "        padded_segments = []\n",
    "        for tokens in tokenized_segments:\n",
    "            padded_tokens = torch.cat([tokens, torch.zeros(1, max_length - tokens.size(1), dtype=torch.long)], dim=1)\n",
    "            padded_segments.append(padded_tokens)\n",
    "\n",
    "        return padded_segments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = row[\"code\"]\n",
    "        # cwe_id_str = row[\"flaw\"]  # Assuming it's in the format \"cwe-XXX\"\n",
    "        \n",
    "        # Extract the integer part of the CWE ID\n",
    "        # cwe_id = int(cwe_id_str.split('-')[-1])\n",
    "\n",
    "        # Remove comments from the code\n",
    "        cleaned_code = self.remove_comments(code)\n",
    "\n",
    "        # Tokenize the cleaned code, including handling long code\n",
    "        tokenized_code_segments = self.tokenize_long_code(cleaned_code)\n",
    "\n",
    "        # Combine and pad code segments to create a single input\n",
    "        combined_input = torch.cat(tokenized_code_segments, dim=1)\n",
    "\n",
    "        # Trim or pad the sequence to the specified max_seq_length\n",
    "        if combined_input.size(1) > self.max_seq_length:\n",
    "            combined_input = combined_input[:, :self.max_seq_length]\n",
    "        else:\n",
    "            padding = torch.zeros(1, self.max_seq_length - combined_input.size(1), dtype=torch.long)\n",
    "            combined_input = torch.cat([combined_input, padding], dim=1)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": combined_input.squeeze(),\n",
    "            \"flaw\": cwe_id,  # Now cwe_id is an integer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'testcase_ID', 'filename', 'code', 'flaw', 'flaw_loc',\n",
       "       'bug'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = new_df.drop([\"Unnamed: 0\",\"testcase_ID\",\"filename\",\"flaw_loc\",\"bug\",\"flaw\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>/* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     code\n",
       "count                                                1000\n",
       "unique                                               1000\n",
       "top     /* TEMPLATE GENERATED TESTCASE FILE\\nFilename:...\n",
       "freq                                                    1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VulnerabilityDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = VulnerabilityDataset(val_data)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * num_epochs)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,  # Adjust the batch size as needed\n",
    "    output_dir=\"./bert_fine_tuned_model_b16\",\n",
    "    num_train_epochs=num_epochs,  # Adjust the number of epochs as needed\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "class CustomDataCollator(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        super().__init__(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=True,\n",
    "            mlm_probability=0.15,\n",
    "        )\n",
    "        self.model = model\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in batch])\n",
    "        attention_mask = torch.stack([example[\"attention_mask\"] for example in batch])\n",
    "        labels = torch.stack([example[\"input_ids\"] for example in batch])  # Self-prediction task: labels are the same as input\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=CustomDataCollator(tokenizer, model), \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset = val_dataset,# Use the DataLoader directly\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1663146e214b3e81ff8d646141b78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7883, 'learning_rate': 4.9e-05, 'epoch': 0.2}\n",
      "{'loss': 1.5065, 'learning_rate': 4.8e-05, 'epoch': 0.4}\n",
      "{'loss': 0.9807, 'learning_rate': 4.7e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7234, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5654, 'learning_rate': 4.5e-05, 'epoch': 1.0}\n",
      "{'loss': 0.5836, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.2}\n",
      "{'loss': 0.46, 'learning_rate': 4.3e-05, 'epoch': 1.4}\n",
      "{'loss': 0.4231, 'learning_rate': 4.2e-05, 'epoch': 1.6}\n",
      "{'loss': 0.3406, 'learning_rate': 4.1e-05, 'epoch': 1.8}\n",
      "{'loss': 0.3826, 'learning_rate': 4e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5415426cbc724b8281a2d0284f0371fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22164955735206604, 'eval_runtime': 56.4899, 'eval_samples_per_second': 3.54, 'eval_steps_per_second': 0.443, 'epoch': 2.0}\n",
      "{'loss': 0.2957, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.2}\n",
      "{'loss': 0.3453, 'learning_rate': 3.8e-05, 'epoch': 2.4}\n",
      "{'loss': 0.2422, 'learning_rate': 3.7e-05, 'epoch': 2.6}\n",
      "{'loss': 0.2112, 'learning_rate': 3.6e-05, 'epoch': 2.8}\n",
      "{'loss': 0.228, 'learning_rate': 3.5e-05, 'epoch': 3.0}\n",
      "{'loss': 0.2373, 'learning_rate': 3.4000000000000007e-05, 'epoch': 3.2}\n",
      "{'loss': 0.1817, 'learning_rate': 3.3e-05, 'epoch': 3.4}\n",
      "{'loss': 0.1659, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.6}\n",
      "{'loss': 0.1995, 'learning_rate': 3.1e-05, 'epoch': 3.8}\n",
      "{'loss': 0.1825, 'learning_rate': 3e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7da810322474f83bc02830c757c1066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07960135489702225, 'eval_runtime': 61.9094, 'eval_samples_per_second': 3.231, 'eval_steps_per_second': 0.404, 'epoch': 4.0}\n",
      "{'loss': 0.176, 'learning_rate': 2.9e-05, 'epoch': 4.2}\n",
      "{'loss': 0.1666, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.4}\n",
      "{'loss': 0.1517, 'learning_rate': 2.7000000000000002e-05, 'epoch': 4.6}\n",
      "{'loss': 0.168, 'learning_rate': 2.6000000000000002e-05, 'epoch': 4.8}\n",
      "{'loss': 0.1742, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n",
      "{'loss': 0.1301, 'learning_rate': 2.4e-05, 'epoch': 5.2}\n",
      "{'loss': 0.1618, 'learning_rate': 2.3000000000000003e-05, 'epoch': 5.4}\n",
      "{'loss': 0.144, 'learning_rate': 2.2000000000000003e-05, 'epoch': 5.6}\n",
      "{'loss': 0.1196, 'learning_rate': 2.1e-05, 'epoch': 5.8}\n",
      "{'loss': 0.1074, 'learning_rate': 2e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da74ab61b4574f8094718558d5a5c832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06423148512840271, 'eval_runtime': 60.5336, 'eval_samples_per_second': 3.304, 'eval_steps_per_second': 0.413, 'epoch': 6.0}\n",
      "{'loss': 0.1213, 'learning_rate': 1.9e-05, 'epoch': 6.2}\n",
      "{'loss': 0.1082, 'learning_rate': 1.8e-05, 'epoch': 6.4}\n",
      "{'loss': 0.099, 'learning_rate': 1.7000000000000003e-05, 'epoch': 6.6}\n",
      "{'loss': 0.0907, 'learning_rate': 1.6000000000000003e-05, 'epoch': 6.8}\n",
      "{'loss': 0.0947, 'learning_rate': 1.5e-05, 'epoch': 7.0}\n",
      "{'loss': 0.1299, 'learning_rate': 1.4000000000000001e-05, 'epoch': 7.2}\n",
      "{'loss': 0.1025, 'learning_rate': 1.3000000000000001e-05, 'epoch': 7.4}\n",
      "{'loss': 0.0757, 'learning_rate': 1.2e-05, 'epoch': 7.6}\n",
      "{'loss': 0.0959, 'learning_rate': 1.1000000000000001e-05, 'epoch': 7.8}\n",
      "{'loss': 0.0838, 'learning_rate': 1e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7452dfb3372242e995fa7f510fb7fece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07428160309791565, 'eval_runtime': 61.2137, 'eval_samples_per_second': 3.267, 'eval_steps_per_second': 0.408, 'epoch': 8.0}\n",
      "{'loss': 0.0768, 'learning_rate': 9e-06, 'epoch': 8.2}\n",
      "{'loss': 0.0903, 'learning_rate': 8.000000000000001e-06, 'epoch': 8.4}\n",
      "{'loss': 0.0836, 'learning_rate': 7.000000000000001e-06, 'epoch': 8.6}\n",
      "{'loss': 0.0589, 'learning_rate': 6e-06, 'epoch': 8.8}\n",
      "{'loss': 0.0993, 'learning_rate': 5e-06, 'epoch': 9.0}\n",
      "{'loss': 0.0662, 'learning_rate': 4.000000000000001e-06, 'epoch': 9.2}\n",
      "{'loss': 0.1017, 'learning_rate': 3e-06, 'epoch': 9.4}\n",
      "{'loss': 0.0875, 'learning_rate': 2.0000000000000003e-06, 'epoch': 9.6}\n",
      "{'loss': 0.0976, 'learning_rate': 1.0000000000000002e-06, 'epoch': 9.8}\n",
      "{'loss': 0.1108, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281a61255bff4d9692b4b88c25439174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05617446452379227, 'eval_runtime': 61.3213, 'eval_samples_per_second': 3.262, 'eval_steps_per_second': 0.408, 'epoch': 10.0}\n",
      "{'train_runtime': 7684.9701, 'train_samples_per_second': 1.041, 'train_steps_per_second': 0.065, 'train_loss': 0.2883510800600052, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.2883510800600052, metrics={'train_runtime': 7684.9701, 'train_samples_per_second': 1.041, 'train_steps_per_second': 0.065, 'train_loss': 0.2883510800600052, 'epoch': 10.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
